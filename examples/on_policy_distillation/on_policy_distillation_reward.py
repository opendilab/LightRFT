"""
On-Policy Distillation Reward Function for LightRFT

This module provides a custom reward function that queries a teacher model
to obtain log probabilities for knowledge distillation during RL training.

The teacher model runs as a separate inference server (vLLM or SGLang),
and this function queries it to get token-level log probabilities for
the sequences generated by the student model.
"""

import asyncio
import aiohttp
import torch
import numpy as np
from typing import List, Dict, Any, Optional


async def get_teacher_logprobs_async(
    url: str,
    sequences: List[str],
    session: Optional[aiohttp.ClientSession] = None
) -> List[Dict[str, Any]]:
    """
    Asynchronously query teacher model for log probabilities.

    :param url: URL of the teacher model inference server
    :type url: str
    :param sequences: List of full sequences (prompt + response)
    :type sequences: List[str]
    :param session: Optional aiohttp session for connection reuse
    :type session: Optional[aiohttp.ClientSession]
    :return: List of response dictionaries containing log probabilities
    :rtype: List[Dict[str, Any]]
    """
    should_close_session = session is None
    if session is None:
        session = aiohttp.ClientSession()

    try:
        tasks = []
        for sequence in sequences:
            payload = {
                "text": sequence,
                "sampling_params": {
                    "temperature": 0,
                    "max_tokens": 0,  # No new generation, just logprobs
                    "skip_special_tokens": False,
                },
                "return_logprob": True,
                "logprob_start_len": 0,  # Get logprobs from the beginning
            }
            tasks.append(session.post(url, json=payload))

        responses = await asyncio.gather(*tasks)
        results = []
        for resp in responses:
            resp.raise_for_status()
            results.append(await resp.json())

        return results
    finally:
        if should_close_session:
            await session.close()


def extract_teacher_logprobs(
    teacher_responses: List[Dict[str, Any]],
    response_lengths: List[int],
    device: str = "cpu"
) -> List[torch.Tensor]:
    """
    Extract teacher log probabilities for the response tokens only.

    :param teacher_responses: List of teacher model API responses
    :type teacher_responses: List[Dict[str, Any]]
    :param response_lengths: Number of response tokens for each sequence
    :type response_lengths: List[int]
    :param device: Target device for tensors
    :type device: str
    :return: List of tensors containing teacher log probs for response tokens
    :rtype: List[torch.Tensor]
    """
    teacher_log_probs_list = []

    for response, response_length in zip(teacher_responses, response_lengths):
        # Extract log probabilities from teacher response
        # The format depends on the inference server (vLLM/SGLang)
        if "meta_info" in response and "input_token_logprobs" in response["meta_info"]:
            # SGLang format
            logprobs = response["meta_info"]["input_token_logprobs"]
            # logprobs is a list of [logprob, rank, decoded_token] tuples
            # Extract just the logprob values
            logprob_values = [item[0] if isinstance(item, list) else item for item in logprobs]
            # Skip the first token (it doesn't have a logprob) and take last response_length tokens
            teacher_log_probs = torch.tensor(logprob_values[1:], dtype=torch.float32)
            teacher_log_probs = teacher_log_probs[-response_length:]
        elif "prompt_logprobs" in response or "token_logprobs" in response:
            # vLLM format
            logprobs = response.get("token_logprobs", response.get("prompt_logprobs", []))
            # Filter out None values and convert to tensor
            logprob_values = [lp for lp in logprobs if lp is not None]
            teacher_log_probs = torch.tensor(logprob_values, dtype=torch.float32)
            teacher_log_probs = teacher_log_probs[-response_length:]
        else:
            raise ValueError(
                f"Unknown response format from teacher model. "
                f"Expected 'meta_info' (SGLang) or 'token_logprobs' (vLLM). "
                f"Got keys: {response.keys()}"
            )

        teacher_log_probs_list.append(teacher_log_probs.to(device))

    return teacher_log_probs_list


def reward_func(queries: List[str], prompts: List[str], **kwargs) -> torch.Tensor:
    """
    Custom reward function for on-policy distillation.

    This function is called by LightRFT's experience maker to compute rewards.
    It queries the teacher model and returns a placeholder reward tensor.
    The actual teacher log probs are stored separately and used by the
    OnPolicyDistillationCalculator.

    :param queries: List of full sequences (prompt + response)
    :type queries: List[str]
    :param prompts: List of prompts (unused, for compatibility)
    :type prompts: List[str]
    :return: Placeholder reward tensor (zeros)
    :rtype: torch.Tensor
    """
    # Return placeholder rewards
    # The actual advantage computation happens in OnPolicyDistillationCalculator
    # using teacher log probs stored in experience.info
    return torch.zeros(len(queries), dtype=torch.float32)


async def get_teacher_logprobs_for_experiences(
    teacher_url: str,
    sequences: List[str],
    response_lengths: List[int],
    device: str = "cpu"
) -> torch.Tensor:
    """
    Get teacher log probabilities for a batch of sequences.

    This is the main entry point for obtaining teacher log probs during training.

    :param teacher_url: URL of the teacher model server
    :type teacher_url: str
    :param sequences: List of full sequences (prompt + response)
    :type sequences: List[str]
    :param response_lengths: Number of response tokens for each sequence
    :type response_lengths: List[int]
    :param device: Target device for tensors
    :type device: str
    :return: Tensor of teacher log probs, padded to match response lengths
    :rtype: torch.Tensor
    """
    # Query teacher model
    responses = await get_teacher_logprobs_async(teacher_url, sequences)

    # Extract log probs
    teacher_log_probs_list = extract_teacher_logprobs(responses, response_lengths, device)

    # Pad to uniform length if needed
    max_length = max(response_lengths)
    padded_log_probs = []
    for log_probs, response_length in zip(teacher_log_probs_list, response_lengths):
        if len(log_probs) < max_length:
            padding = torch.zeros(max_length - len(log_probs), dtype=torch.float32, device=device)
            log_probs = torch.cat([log_probs, padding])
        padded_log_probs.append(log_probs)

    return torch.stack(padded_log_probs)


# Synchronous wrapper for compatibility with LightRFT
def get_teacher_logprobs_sync(
    teacher_url: str,
    sequences: List[str],
    response_lengths: List[int],
    device: str = "cpu"
) -> torch.Tensor:
    """
    Synchronous wrapper for getting teacher log probabilities.

    :param teacher_url: URL of the teacher model server
    :type teacher_url: str
    :param sequences: List of full sequences (prompt + response)
    :type sequences: List[str]
    :param response_lengths: Number of response tokens for each sequence
    :type response_lengths: List[int]
    :param device: Target device for tensors
    :type device: str
    :return: Tensor of teacher log probs
    :rtype: torch.Tensor
    """
    return asyncio.run(
        get_teacher_logprobs_for_experiences(teacher_url, sequences, response_lengths, device)
    )
