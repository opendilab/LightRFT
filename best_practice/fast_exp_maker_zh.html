


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!-->
<html class="no-js" lang="en">
<!--<![endif]-->

<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>FastExperienceMaker 最佳实践指南 &mdash; LightRFT v0.1.1 documentation</title>
  

  <link rel="shortcut icon" href="../_static/images/favicon.ico" />
  
  

  

  
  
  

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/readthedocs.css" type="text/css" />
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <!-- Google Analytics -->
  <script type="text/javascript">
    var collapsedSections = [];
  </script>
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script>
    MathJax = {
        chtml: {
            scale: 1,
            minScale: 1,
        },
        svg: {
            scale: 1,
            minScale: 1,
        }
    }
</script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"
    integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/"
        aria-label="OpenMMLab"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
          <li >
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a
                class="resource-option with-down-arrow">
                OpenDILab
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-engine" target="_blank">
                  <span class="dropdown-title">DI-engine </span>
                  <p>OpenDILab Decision AI Engine</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/LightZero" target="_blank">
                  <span class="dropdown-title">LightZero </span>
                  <p>OpenDILab Decision Monte Carlo Tree Search Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GenerativeRL" target="_blank">
                  <span class="dropdown-title">GenerativeRL </span>
                  <p>OpenDILab Generative AI Framework</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-star" target="_blank">
                  <span class="dropdown-title">DI-star </span>
                  <p>OpenDILab Decision AI in StarCraftII</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-drive" target="_blank">
                  <span class="dropdown-title">DI-drive </span>
                  <p>OpenDILab Auto-driving platform</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/GoBigger" target="_blank">
                  <span class="dropdown-title">GoBigger </span>
                  <p>OpenDILab Multi-Agent Environment</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-smartcross" target="_blank">
                  <span class="dropdown-title">DI-smartcross </span>
                  <p>Decision Intelligence Platform for Traffic Crossing Signal Control</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-treetensor" target="_blank">
                  <span class="dropdown-title">DI-treetensor </span>
                  <p>Tree Nested PyTorch Tensor Lib</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/DI-sheep" target="_blank">
                  <span class="dropdown-title">DI-sheep </span>
                  <p>Deep Reinforcement Learning + 3 Tiles Game</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-model-based-RL" target="_blank">
                  <span class="dropdown-title">awesome-model-based-RL </span>
                  <p>A curated list of awesome model based RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-decision-transformer" target="_blank">
                  <span class="dropdown-title">awesome-decision-transformer </span>
                  <p>A curated list of Decision Transformer resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-exploration-rl" target="_blank">
                  <span class="dropdown-title">awesome-exploration-rl </span>
                  <p>A curated list of awesome exploration RL resources (continually updated)</p>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item"
                  href="https://github.com/opendilab/awesome-multi-modal-reinforcement-learning" target="_blank">
                  <span class="dropdown-title">awesome-multi-modal-reinforcement-learning </span>
                  <p>A curated list of Multi-Modal Reinforcement Learning resources (continually updated)</p>
                </a>
              </div>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

  

  <div class="table-of-contents-link-wrapper">
    <span>Table of Contents</span>
    <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
  </div>

  <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
    <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <div class="pytorch-left-menu-search">
          

          
          
          
          

          



<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        
        
        
        
        
        <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quick_start/index.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide &amp; Best Practices</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html">Best Practices</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/utils/index.html">lightrft.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/datasets/index.html">lightrft.datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/models/index.html">lightrft.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/strategy/index.html">lightrft.strategy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc/trainer/index.html">lightrft.trainer</a></li>
</ul>

        
        
      </div>
    </div>
  </nav>

  <div class="pytorch-container">
    <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
      <div class="pytorch-breadcrumbs-wrapper">
        















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
            Docs
        </a> &gt;
      </li>

        
      <li>FastExperienceMaker 最佳实践指南</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/best_practice/fast_exp_maker_zh.md.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
      </div>

      <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
        Shortcuts
      </div>
    </div>

    <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
      <div class="pytorch-content-left">
        
          <div class="rst-content">
            
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
              <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                
  <section id="fastexperiencemaker">
<h1>FastExperienceMaker 最佳实践指南<a class="headerlink" href="#fastexperiencemaker" title="Permalink to this heading">¶</a></h1>
<section id="id1">
<h2>目录<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>概述</p></li>
<li><p>核心功能</p></li>
<li><p>架构组件</p></li>
<li><p>使用指南</p></li>
<li><p>配置参数</p></li>
<li><p>优势估计方法</p></li>
<li><p>最佳实践</p></li>
<li><p>常见问题与解决方案</p></li>
<li><p>性能调优</p></li>
</ul>
</section>
<section id="id2">
<h2>概述<a class="headerlink" href="#id2" title="Permalink to this heading">¶</a></h2>
<section id="id3">
<h3>什么是 FastExperienceMaker？<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h3>
<p>FastExperienceMaker 是 LightRFT 中用于 RLHF（从人类反馈中强化学习）训练的优化经验生成引擎。它扩展了基础的 <code class="docutils literal notranslate"><span class="pre">NaiveExperienceMaker</span></code>，支持高性能推理后端（VLLM/SGLang）和高级强化学习特性。</p>
</section>
<section id="id4">
<h3>核心能力<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>高性能推理</strong>：支持 VLLM 和 SGLang 后端，实现高效文本生成</p></li>
<li><p><strong>多模态支持</strong>：支持视觉-语言模型（VLM）的图像和视频数据处理</p></li>
<li><p><strong>高级优势估计</strong>：支持多种方法，包括 GAE、RLOO、REINFORCE 和 Group Normalization</p></li>
<li><p><strong>灵活的奖励组合</strong>：支持多个奖励模型和自定义聚合函数</p></li>
<li><p><strong>样本打包</strong>：通过序列打包提高训练效率</p></li>
<li><p><strong>奖励归一化</strong>：运行时奖励统计，支持归一化和裁剪</p></li>
</ul>
</section>
</section>
<section id="id5">
<h2>核心功能<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h2>
<section id="id6">
<h3>1. 经验生成流程<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h3>
<p>FastExperienceMaker 实现了一个 7 阶段的经验生成流程：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>阶段 1: 样本生成 (VLLM/SGLang)
    ↓
阶段 2: 分片并行预处理
    ↓
阶段 3: 模型推理 (Actor, Critic, Initial, Reward Models)
    ↓
阶段 4: 分片并行后处理
    ↓
阶段 5: 奖励处理 (归一化、塑形、过滤)
    ↓
阶段 6: 多图像/视频处理
    ↓
阶段 7: 优势计算
</pre></div>
</div>
</section>
<section id="id7">
<h3>2. 多模态数据处理<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">MultimodalDataProcessor</span></code> 类处理纯文本和图像-文本混合数据：</p>
<ul class="simple">
<li><p><strong>自动分离</strong>：分离纯文本和多模态样本</p></li>
<li><p><strong>适当处理</strong>：通过分词器或多模态处理器路由样本</p></li>
<li><p><strong>顺序保持</strong>：处理后保持原始批次顺序</p></li>
<li><p><strong>多图像/视频支持</strong>：处理每个样本的多个图像或视频</p></li>
</ul>
</section>
<section id="id8">
<h3>3. 奖励计算引擎<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">RewardComputationEngine</span></code> 管理奖励模型推理和聚合：</p>
<ul class="simple">
<li><p><strong>远程奖励模型</strong>：支持 HTTP/gRPC 奖励模型</p></li>
<li><p><strong>本地奖励模型</strong>：基于 PyTorch 的奖励模型</p></li>
<li><p><strong>自定义奖励函数</strong>：用于自定义奖励逻辑的 Python 函数</p></li>
<li><p><strong>多模型集成</strong>：使用自定义聚合组合多个奖励模型</p></li>
<li><p><strong>优化批处理</strong>：高效的批处理，支持可选的样本过滤</p></li>
</ul>
</section>
</section>
<section id="id9">
<h2>架构组件<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h2>
<section id="id10">
<h3>类层次结构<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>NaiveExperienceMaker (基类)
    ↓
FastExperienceMaker
    ├── MultimodalDataProcessor
    ├── RewardComputationEngine
    └── AdvantageCalculator (GAE/RLOO/REINFORCE/GroupNorm)
</pre></div>
</div>
</section>
<section id="id11">
<h3>关键类<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h3>
<section id="id12">
<h4>1. FastExperienceMaker<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h4>
<p><strong>用途</strong>：主要的经验生成类，具有优化推理和高级强化学习特性。</p>
<p><strong>初始化参数</strong>：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">packing_samples</span></code> (bool)：启用样本打包以提高效率</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">processor</span></code>：用于 VLM 模型的多模态处理器</p></li>
<li><p>其他参数继承自 <code class="docutils literal notranslate"><span class="pre">NaiveExperienceMaker</span></code></p></li>
</ul>
<p><strong>关键方法</strong>：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">make_experience_list()</span></code>：从提示生成经验</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">generate_samples()</span></code>：使用推理引擎生成样本</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_advantages_and_returns()</span></code>：计算优势和回报</p></li>
</ul>
</section>
<section id="multimodaldataprocessor">
<h4>2. MultimodalDataProcessor<a class="headerlink" href="#multimodaldataprocessor" title="Permalink to this heading">¶</a></h4>
<p><strong>用途</strong>：处理纯文本和多模态混合数据的预处理。</p>
<p><strong>主要职责</strong>：</p>
<ul class="simple">
<li><p>归一化图像/视频输入（文件路径、PIL 图像、字节）</p></li>
<li><p>分离纯文本和多模态样本</p></li>
<li><p>通过适当的流程处理</p></li>
<li><p>按 <code class="docutils literal notranslate"><span class="pre">n_samples_per_prompt</span></code> 因子扩展样本</p></li>
</ul>
</section>
<section id="rewardcomputationengine">
<h4>3. RewardComputationEngine<a class="headerlink" href="#rewardcomputationengine" title="Permalink to this heading">¶</a></h4>
<p><strong>用途</strong>：管理奖励模型推理和分数聚合。</p>
<p><strong>处理流程</strong>：</p>
<ol class="arabic simple">
<li><p><strong>收集</strong>：根据奖励配方收集或过滤样本</p></li>
<li><p><strong>处理</strong>：通过奖励模型运行前向传播</p></li>
<li><p><strong>聚合</strong>：使用 reward_fn 组合分数</p></li>
</ol>
</section>
</section>
</section>
<section id="id13">
<h2>使用指南<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h2>
<section id="id14">
<h3>基础用法<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h3>
<section id="id15">
<h4>纯文本生成<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">lightrft.trainer.fast_exp_maker</span><span class="w"> </span><span class="kn">import</span> <span class="n">FastExperienceMaker</span>

<span class="c1"># 初始化经验生成器</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">actor</span><span class="o">=</span><span class="n">actor_model</span><span class="p">,</span>
    <span class="n">critic</span><span class="o">=</span><span class="n">critic_model</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">initial_model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">prompt_max_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">kl_controller</span><span class="o">=</span><span class="n">kl_controller</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
    <span class="n">packing_samples</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 生成经验</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;解释量子计算&quot;</span><span class="p">,</span> <span class="s2">&quot;什么是机器学习？&quot;</span><span class="p">]</span>
<span class="n">experiences</span> <span class="o">=</span> <span class="n">exp_maker</span><span class="o">.</span><span class="n">make_experience_list</span><span class="p">(</span>
    <span class="n">all_prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id16">
<h4>视觉-语言生成<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># 使用处理器初始化以支持 VLM</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">actor</span><span class="o">=</span><span class="n">actor_model</span><span class="p">,</span>
    <span class="n">critic</span><span class="o">=</span><span class="n">critic_model</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">initial_model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">processor</span><span class="o">=</span><span class="n">multimodal_processor</span><span class="p">,</span>  <span class="c1"># VLM 必需</span>
    <span class="n">prompt_max_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">kl_controller</span><span class="o">=</span><span class="n">kl_controller</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 准备多模态数据</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;描述这张图片&quot;</span><span class="p">,</span> <span class="s2">&quot;图片里有什么？&quot;</span><span class="p">]</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;image1.jpg&quot;</span><span class="p">)],</span>  <span class="c1"># 单张图片</span>
    <span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;img2.jpg&quot;</span><span class="p">),</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;img3.jpg&quot;</span><span class="p">)],</span>  <span class="c1"># 多张图片</span>
<span class="p">]</span>
<span class="n">references</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;沙发上的一只猫&quot;</span><span class="p">,</span> <span class="s2">&quot;两只狗在玩耍&quot;</span><span class="p">]</span>

<span class="c1"># 生成经验</span>
<span class="n">experiences</span> <span class="o">=</span> <span class="n">exp_maker</span><span class="o">.</span><span class="n">make_experience_list</span><span class="p">(</span>
    <span class="n">all_prompts</span><span class="o">=</span><span class="n">prompts</span><span class="p">,</span>
    <span class="n">all_images</span><span class="o">=</span><span class="n">images</span><span class="p">,</span>
    <span class="n">all_references</span><span class="o">=</span><span class="n">references</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id17">
<h3>高级用法<a class="headerlink" href="#id17" title="Permalink to this heading">¶</a></h3>
<section id="id18">
<h4>多奖励模型与自定义聚合<a class="headerlink" href="#id18" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 定义自定义奖励聚合函数</span>
<span class="k">def</span><span class="w"> </span><span class="nf">custom_reward_fn</span><span class="p">(</span><span class="n">model_reward_list</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">refs</span><span class="p">,</span> <span class="n">label_map</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    自定义奖励聚合函数。</span>

<span class="sd">    参数：</span>
<span class="sd">        model_reward_list: 每个模型的奖励张量列表</span>
<span class="sd">        labels: 样本标签</span>
<span class="sd">        queries: 生成的文本</span>
<span class="sd">        refs: 参考文本</span>
<span class="sd">        label_map: 奖励模型名称到索引的映射</span>

<span class="sd">    返回：</span>
<span class="sd">        aggregated_rewards: 组合的奖励张量</span>
<span class="sd">        reward_metrics: 详细指标字典</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># 示例：基于标签的加权平均</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">])</span>  <span class="c1"># 两个模型的权重</span>
    <span class="n">aggregated</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">*</span> <span class="n">r</span> <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">model_reward_list</span><span class="p">))</span>

    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;reward_model_1&quot;</span><span class="p">:</span> <span class="n">model_reward_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
        <span class="s2">&quot;reward_model_2&quot;</span><span class="p">:</span> <span class="n">model_reward_list</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="n">aggregated</span><span class="p">,</span> <span class="n">metrics</span>

<span class="c1"># 使用多个奖励模型初始化</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">actor</span><span class="o">=</span><span class="n">actor_model</span><span class="p">,</span>
    <span class="n">critic</span><span class="o">=</span><span class="n">critic_model</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="p">[</span><span class="n">reward_model_1</span><span class="p">,</span> <span class="n">reward_model_2</span><span class="p">],</span>  <span class="c1"># 模型列表</span>
    <span class="n">reward_fn</span><span class="o">=</span><span class="n">custom_reward_fn</span><span class="p">,</span>
    <span class="n">reward_fn_label_map</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;rm1&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;rm2&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">initial_model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id19">
<h4>样本打包以提高效率<a class="headerlink" href="#id19" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 启用样本打包</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">actor</span><span class="o">=</span><span class="n">actor_model</span><span class="p">,</span>
    <span class="n">critic</span><span class="o">=</span><span class="n">critic_model</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="n">reward_model</span><span class="p">,</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">initial_model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
    <span class="n">packing_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 启用打包</span>
<span class="p">)</span>

<span class="c1"># 打包格式：| prompt1 response1 [EOS] | prompt2 response2 [EOS] | ...</span>
<span class="c1"># 优势：</span>
<span class="c1"># - 减少填充开销</span>
<span class="c1"># - 提高 GPU 利用率</span>
<span class="c1"># - 更快的训练吞吐量</span>
</pre></div>
</div>
</section>
<section id="id20">
<h4>远程奖励模型<a class="headerlink" href="#id20" title="Permalink to this heading">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 通过 HTTP/gRPC 使用远程奖励模型</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">actor</span><span class="o">=</span><span class="n">actor_model</span><span class="p">,</span>
    <span class="n">critic</span><span class="o">=</span><span class="n">critic_model</span><span class="p">,</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># 无本地奖励模型</span>
    <span class="n">remote_rm_url</span><span class="o">=</span><span class="p">[</span>
        <span class="s2">&quot;http://reward-server-1:8000/score&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://reward-server-2:8000/score&quot;</span><span class="p">,</span>
    <span class="p">],</span>
    <span class="n">initial_model</span><span class="o">=</span><span class="n">initial_model</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="id21">
<h2>配置参数<a class="headerlink" href="#id21" title="Permalink to this heading">¶</a></h2>
<section id="id22">
<h3>生成参数<a class="headerlink" href="#id22" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>描述</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>采样温度（越高越随机）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">top_p</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>核采样阈值</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">top_k</span></code></p></td>
<td><p>int</p></td>
<td><p>-1</p></td>
<td><p>Top-k 采样（-1 = 禁用）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></p></td>
<td><p>int</p></td>
<td><p>1024</p></td>
<td><p>生成的最大令牌数</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">min_new_tokens</span></code></p></td>
<td><p>int</p></td>
<td><p>1</p></td>
<td><p>生成的最小令牌数</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">skip_special_tokens</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>输出中跳过特殊令牌</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id23">
<h3>奖励处理参数<a class="headerlink" href="#id23" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>描述</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">reward_running_norm</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>启用运行时奖励归一化</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">reward_running_norm_minus_mean</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>归一化时减去均值</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">reward_clip</span></code></p></td>
<td><p>float</p></td>
<td><p>0</p></td>
<td><p>奖励裁剪阈值（0 = 禁用）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">overlong_buffer</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>启用过长序列惩罚</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">overlong_buffer_len</span></code></p></td>
<td><p>int</p></td>
<td><p>50</p></td>
<td><p>过长惩罚的缓冲区长度</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">overlong_buffer_penalty_factor</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>过长序列的惩罚因子</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id24">
<h3>优势估计参数<a class="headerlink" href="#id24" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>类型</p></th>
<th class="head"><p>默认值</p></th>
<th class="head"><p>描述</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">advantage_estimator</span></code></p></td>
<td><p>str</p></td>
<td><p>“gae”</p></td>
<td><p>方法：”gae”、”rloo”、”reinforce”、”group_norm”</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">advantages_norm</span></code></p></td>
<td><p>bool</p></td>
<td><p>False</p></td>
<td><p>启用优势归一化（白化）</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">advantage_clip</span></code></p></td>
<td><p>float</p></td>
<td><p>0</p></td>
<td><p>优势裁剪阈值（0 = 禁用）</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">gamma</span></code></p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>回报的折扣因子</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">lambd</span></code></p></td>
<td><p>float</p></td>
<td><p>0.95</p></td>
<td><p>GAE lambda 参数</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id25">
<h2>优势估计方法<a class="headerlink" href="#id25" title="Permalink to this heading">¶</a></h2>
<section id="gae">
<h3>1. GAE（广义优势估计）<a class="headerlink" href="#gae" title="Permalink to this heading">¶</a></h3>
<p><strong>何时使用</strong>：标准 PPO 训练，需要 critic 模型。</p>
<p><strong>优势</strong>：</p>
<ul class="simple">
<li><p>通过 lambda 参数平衡偏差-方差权衡</p></li>
<li><p>平滑的优势估计</p></li>
<li><p>与价值函数配合良好</p></li>
</ul>
<p><strong>配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;gae&quot;</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.95</span>
</pre></div>
</div>
</section>
<section id="rloo-reinforce-leave-one-out">
<h3>2. RLOO（REINFORCE Leave-One-Out）<a class="headerlink" href="#rloo-reinforce-leave-one-out" title="Permalink to this heading">¶</a></h3>
<p><strong>何时使用</strong>：无 critic 模型训练，每个提示生成多个样本。</p>
<p><strong>优势</strong>：</p>
<ul class="simple">
<li><p>不需要 critic 模型</p></li>
<li><p>通过基线减法减少方差</p></li>
<li><p>对每个提示的多个样本高效</p></li>
</ul>
<p><strong>配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;rloo&quot;</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_samples_per_prompt</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 必须 &gt; 1</span>
</pre></div>
</div>
</section>
<section id="reinforce-with-baseline">
<h3>3. REINFORCE with Baseline<a class="headerlink" href="#reinforce-with-baseline" title="Permalink to this heading">¶</a></h3>
<p><strong>何时使用</strong>：简单的策略梯度，使用奖励基线。</p>
<p><strong>优势</strong>：</p>
<ul class="simple">
<li><p>简单直接</p></li>
<li><p>适用于每个提示的单个样本</p></li>
<li><p>不需要 critic 模型</p></li>
</ul>
<p><strong>配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;reinforce&quot;</span>
</pre></div>
</div>
</section>
<section id="group-normalization-grpo">
<h3>4. Group Normalization (GRPO)<a class="headerlink" href="#group-normalization-grpo" title="Permalink to this heading">¶</a></h3>
<p><strong>何时使用</strong>：基于组的优势归一化，每个提示生成多个样本。</p>
<p><strong>优势</strong>：</p>
<ul class="simple">
<li><p>在每个提示组内归一化优势</p></li>
<li><p>减少不同提示之间的方差</p></li>
<li><p>对多样化的提示分布有效</p></li>
</ul>
<p><strong>配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;group_norm&quot;</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_samples_per_prompt</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 必须 &gt; 1</span>
</pre></div>
</div>
</section>
<section id="id26">
<h3>方法对比表<a class="headerlink" href="#id26" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>方法</p></th>
<th class="head"><p>需要 Critic</p></th>
<th class="head"><p>每个提示的样本数</p></th>
<th class="head"><p>方差</p></th>
<th class="head"><p>偏差</p></th>
<th class="head"><p>复杂度</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>GAE</p></td>
<td><p>是</p></td>
<td><p>任意</p></td>
<td><p>低</p></td>
<td><p>低</p></td>
<td><p>中等</p></td>
</tr>
<tr class="row-odd"><td><p>RLOO</p></td>
<td><p>否</p></td>
<td><p>&gt; 1</p></td>
<td><p>中等</p></td>
<td><p>低</p></td>
<td><p>低</p></td>
</tr>
<tr class="row-even"><td><p>REINFORCE</p></td>
<td><p>否</p></td>
<td><p>任意</p></td>
<td><p>高</p></td>
<td><p>低</p></td>
<td><p>低</p></td>
</tr>
<tr class="row-odd"><td><p>Group Norm</p></td>
<td><p>否</p></td>
<td><p>&gt; 1</p></td>
<td><p>中等</p></td>
<td><p>中等</p></td>
<td><p>低</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="id27">
<h2>最佳实践<a class="headerlink" href="#id27" title="Permalink to this heading">¶</a></h2>
<section id="id28">
<h3>1. 选择推理后端<a class="headerlink" href="#id28" title="Permalink to this heading">¶</a></h3>
<p><strong>VLLM</strong>：</p>
<ul class="simple">
<li><p>✅ 最适合：大规模部署、高吞吐量</p></li>
<li><p>✅ 支持：PagedAttention、连续批处理</p></li>
<li><p>⚠️ 注意：需要 CUDA 兼容的 GPU</p></li>
</ul>
<p><strong>SGLang</strong>：</p>
<ul class="simple">
<li><p>✅ 最适合：研究、灵活性</p></li>
<li><p>✅ 支持：自定义采样、结构化生成</p></li>
<li><p>⚠️ 注意：可能有不同的性能特征</p></li>
</ul>
</section>
<section id="id29">
<h3>2. 多模态数据处理<a class="headerlink" href="#id29" title="Permalink to this heading">¶</a></h3>
<p><strong>图像归一化</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 支持的格式：</span>
<span class="c1"># 1. PIL Image 对象</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;img.jpg&quot;</span><span class="p">)]]</span>

<span class="c1"># 2. 文件路径（将自动加载）</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[[</span><span class="s2">&quot;path/to/image.jpg&quot;</span><span class="p">]]</span>

<span class="c1"># 3. 混合格式</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;img1.jpg&quot;</span><span class="p">),</span> <span class="s2">&quot;path/to/img2.jpg&quot;</span><span class="p">]]</span>
</pre></div>
</div>
<p><strong>多图像场景</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 每个样本多张图片</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="n">img1</span><span class="p">,</span> <span class="n">img2</span><span class="p">,</span> <span class="n">img3</span><span class="p">],</span>  <span class="c1"># 样本 1：3 张图片</span>
    <span class="p">[</span><span class="n">img4</span><span class="p">],</span>              <span class="c1"># 样本 2：1 张图片</span>
    <span class="kc">None</span><span class="p">,</span>                <span class="c1"># 样本 3：无图片（纯文本）</span>
<span class="p">]</span>
</pre></div>
</div>
</section>
<section id="id30">
<h3>3. 奖励模型配置<a class="headerlink" href="#id30" title="Permalink to this heading">¶</a></h3>
<p><strong>单个奖励模型</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="n">single_rm</span><span class="p">,</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>多个奖励模型与聚合</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">reward_model</span><span class="o">=</span><span class="p">[</span><span class="n">rm1</span><span class="p">,</span> <span class="n">rm2</span><span class="p">,</span> <span class="n">rm3</span><span class="p">],</span>
    <span class="n">reward_fn</span><span class="o">=</span><span class="n">custom_aggregation_fn</span><span class="p">,</span>
    <span class="n">reward_fn_label_map</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;quality&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;safety&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;helpfulness&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>自定义奖励函数</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_reward</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;自定义奖励计算逻辑&quot;&quot;&quot;</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">query</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># 你的自定义逻辑</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">compute_custom_score</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rewards</span>

<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">custom_reward_func</span><span class="o">=</span><span class="n">custom_reward</span><span class="p">,</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id31">
<h3>4. 内存优化<a class="headerlink" href="#id31" title="Permalink to this heading">¶</a></h3>
<p><strong>启用样本打包</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 减少 30-50% 的填充开销</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">packing_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>调整微批次大小</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 平衡内存使用和吞吐量</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">micro_rollout_batch_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="c1"># 根据 GPU 内存调整</span>
</pre></div>
</div>
<p><strong>梯度检查点</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 为大型模型启用</span>
<span class="n">actor</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id32">
<h3>5. 奖励归一化策略<a class="headerlink" href="#id32" title="Permalink to this heading">¶</a></h3>
<p><strong>运行时归一化</strong>（推荐用于稳定训练）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">reward_running_norm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">reward_running_norm_minus_mean</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># 减去均值</span>
</pre></div>
</div>
<p><strong>奖励裁剪</strong>（防止异常值）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">reward_clip</span> <span class="o">=</span> <span class="mf">10.0</span>  <span class="c1"># 裁剪到 [-10, 10]</span>
</pre></div>
</div>
<p><strong>优势归一化</strong>（稳定策略更新）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantages_norm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_clip</span> <span class="o">=</span> <span class="mf">5.0</span>  <span class="c1"># 可选裁剪</span>
</pre></div>
</div>
</section>
<section id="id33">
<h3>6. 处理过长序列<a class="headerlink" href="#id33" title="Permalink to this heading">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 惩罚过长的序列</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">overlong_buffer</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">overlong_buffer_len</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># 缓冲区长度</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">overlong_buffer_penalty_factor</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># 惩罚强度</span>

<span class="c1"># 示例：如果 max_new_tokens=512 且 buffer_len=50</span>
<span class="c1"># 预期长度 = 512 - 50 = 462</span>
<span class="c1"># 长度超过 462 个令牌的序列将受到惩罚</span>
</pre></div>
</div>
</section>
</section>
<section id="id34">
<h2>常见问题与解决方案<a class="headerlink" href="#id34" title="Permalink to this heading">¶</a></h2>
<section id="oom">
<h3>问题 1：内存不足（OOM）<a class="headerlink" href="#oom" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：经验生成过程中出现 CUDA 内存不足错误。</p>
<p><strong>解决方案</strong>：</p>
<ol class="arabic simple">
<li><p>启用样本打包：<code class="docutils literal notranslate"><span class="pre">packing_samples=True</span></code></p></li>
<li><p>减小微批次大小：<code class="docutils literal notranslate"><span class="pre">strategy.config.micro_rollout_batch_size</span> <span class="pre">=</span> <span class="pre">4</span></code></p></li>
<li><p>启用梯度检查点：<code class="docutils literal notranslate"><span class="pre">actor.gradient_checkpointing_enable()</span></code></p></li>
<li><p>减少最大序列长度：<code class="docutils literal notranslate"><span class="pre">max_new_tokens=256</span></code></p></li>
<li><p>使用更小的模型或量化</p></li>
</ol>
</section>
<section id="id35">
<h3>问题 2：生成速度慢<a class="headerlink" href="#id35" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：经验生成耗时过长。</p>
<p><strong>解决方案</strong>：</p>
<ol class="arabic simple">
<li><p>使用 VLLM 后端：<code class="docutils literal notranslate"><span class="pre">strategy.args.engine_type</span> <span class="pre">=</span> <span class="pre">&quot;vllm&quot;</span></code></p></li>
<li><p>增加批次大小：<code class="docutils literal notranslate"><span class="pre">strategy.config.micro_rollout_batch_size</span> <span class="pre">=</span> <span class="pre">16</span></code></p></li>
<li><p>启用样本打包：<code class="docutils literal notranslate"><span class="pre">packing_samples=True</span></code></p></li>
<li><p>检查 GPU 利用率：确保 GPU 充分利用</p></li>
<li><p>如果可能，减少 <code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code></p></li>
</ol>
</section>
<section id="id36">
<h3>问题 3：训练不稳定<a class="headerlink" href="#id36" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：训练过程中奖励或损失剧烈波动。</p>
<p><strong>解决方案</strong>：</p>
<ol class="arabic">
<li><p>启用奖励归一化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">reward_running_norm</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">reward_running_norm_minus_mean</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</li>
<li><p>启用优势归一化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantages_norm</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</li>
<li><p>添加奖励裁剪：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">reward_clip</span> <span class="o">=</span> <span class="mf">10.0</span>
</pre></div>
</div>
</li>
<li><p>降低学习率</p></li>
<li><p>使用 GAE 并设置适当的 lambda：<code class="docutils literal notranslate"><span class="pre">strategy.config.lambd</span> <span class="pre">=</span> <span class="pre">0.95</span></code></p></li>
</ol>
</section>
<section id="vlm">
<h3>问题 4：图像令牌不匹配（VLM）<a class="headerlink" href="#vlm" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：推理过程中出现令牌/补丁不匹配的警告消息。</p>
<p><strong>原因</strong>：图像令牌数量与像素值补丁不匹配。</p>
<p><strong>解决方案</strong>：FastExperienceMaker 会自动修复此问题。警告仅供参考。如果频繁出现：</p>
<ol class="arabic simple">
<li><p>检查图像预处理流程</p></li>
<li><p>验证处理器配置</p></li>
<li><p>确保样本之间的图像格式一致</p></li>
</ol>
</section>
<section id="rloo">
<h3>问题 5：RLOO 需要多个样本<a class="headerlink" href="#rloo" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：使用 RLOO 时 <code class="docutils literal notranslate"><span class="pre">n_samples_per_prompt</span> <span class="pre">=</span> <span class="pre">1</span></code> 导致错误。</p>
<p><strong>原因</strong>：RLOO 需要每个提示生成多个样本来计算基线。</p>
<p><strong>解决方案</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 设置 n_samples_per_prompt &gt; 1</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">n_samples_per_prompt</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;rloo&quot;</span>
</pre></div>
</div>
<p>或切换到其他方法：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 改用 GAE 或 REINFORCE</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">advantage_estimator</span> <span class="o">=</span> <span class="s2">&quot;gae&quot;</span>
</pre></div>
</div>
</section>
<section id="id37">
<h3>问题 6：远程奖励模型超时<a class="headerlink" href="#id37" title="Permalink to this heading">¶</a></h3>
<p><strong>症状</strong>：使用远程奖励模型时出现超时错误。</p>
<p><strong>解决方案</strong>：</p>
<ol class="arabic simple">
<li><p>检查到奖励模型服务器的网络连接</p></li>
<li><p>在 remote_rm_fn 配置中增加超时时间</p></li>
<li><p>减小批次大小以避免长时间处理</p></li>
<li><p>考虑使用本地奖励模型以获得更好的性能</p></li>
<li><p>在自定义奖励函数中实现重试逻辑</p></li>
</ol>
</section>
</section>
<section id="id38">
<h2>性能调优<a class="headerlink" href="#id38" title="Permalink to this heading">¶</a></h2>
<section id="id39">
<h3>吞吐量优化<a class="headerlink" href="#id39" title="Permalink to this heading">¶</a></h3>
<p><strong>最大吞吐量的推荐配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">args</span><span class="o">.</span><span class="n">engine_type</span> <span class="o">=</span> <span class="s2">&quot;vllm&quot;</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">micro_rollout_batch_size</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># 根据 GPU 内存调整</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">packing_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>预期性能</strong>：</p>
<ul class="simple">
<li><p>VLLM 后端：比 HuggingFace generate 快 2-5 倍</p></li>
<li><p>样本打包：减少 30-50% 的填充开销</p></li>
<li><p>批处理：批次大小线性扩展（直到 GPU 内存限制）</p></li>
</ul>
</section>
<section id="id40">
<h3>内存效率<a class="headerlink" href="#id40" title="Permalink to this heading">¶</a></h3>
<p><strong>内存受限环境的推荐配置</strong>：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">micro_rollout_batch_size</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">strategy</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">max_new_tokens</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">exp_maker</span> <span class="o">=</span> <span class="n">FastExperienceMaker</span><span class="p">(</span>
    <span class="n">packing_samples</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># ...</span>
<span class="p">)</span>
<span class="n">actor</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="id41">
<h2>参考资料<a class="headerlink" href="#id41" title="Permalink to this heading">¶</a></h2>
<section id="id42">
<h3>相关文档<a class="headerlink" href="#id42" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="strategy_design_philosophy.html"><span class="std std-doc">策略设计哲学</span></a></p></li>
<li><p><a class="reference internal" href="model.html"><span class="std std-doc">模型设计文档</span></a></p></li>
<li><p><a class="reference internal" href="reward_model.html"><span class="std std-doc">奖励模型最佳实践</span></a></p></li>
</ul>
</section>
<section id="id43">
<h3>代码引用<a class="headerlink" href="#id43" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>FastExperienceMaker：<code class="docutils literal notranslate"><span class="pre">lightrft/trainer/fast_exp_maker.py</span></code></p></li>
<li><p>基础 ExperienceMaker：<code class="docutils literal notranslate"><span class="pre">lightrft/trainer/experience_maker.py</span></code></p></li>
<li><p>优势计算器：<code class="docutils literal notranslate"><span class="pre">lightrft/trainer/advantage_calculator.py</span></code></p></li>
<li><p>VLLM 工具：<code class="docutils literal notranslate"><span class="pre">lightrft/strategy/vllm_utils/</span></code></p></li>
</ul>
</section>
<section id="id44">
<h3>研究论文<a class="headerlink" href="#id44" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>GAE</strong>：”High-Dimensional Continuous Control Using Generalized Advantage Estimation”（Schulman 等，2016）</p></li>
<li><p><strong>PPO</strong>：”Proximal Policy Optimization Algorithms”（Schulman 等，2017）</p></li>
<li><p><strong>RLOO</strong>：”Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs”（Ahmadian 等，2024）</p></li>
</ul>
<hr class="docutils" />
<p><strong>文档版本</strong>：1.0
<strong>最后更新</strong>：2026-02-03
<strong>维护者</strong>：LightRFT 团队</p>
</section>
</section>
</section>


              </article>
              
            </div>
            <footer>
  

  <hr>

  <div role="contentinfo">
    <p>
      &copy; Copyright 2025, OpenDILab.

    </p>
  </div>
  
  <div>
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a
      href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the
      Docs</a>.
  </div>
   

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">FastExperienceMaker 最佳实践指南</a><ul>
<li><a class="reference internal" href="#id1">目录</a></li>
<li><a class="reference internal" href="#id2">概述</a><ul>
<li><a class="reference internal" href="#id3">什么是 FastExperienceMaker？</a></li>
<li><a class="reference internal" href="#id4">核心能力</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5">核心功能</a><ul>
<li><a class="reference internal" href="#id6">1. 经验生成流程</a></li>
<li><a class="reference internal" href="#id7">2. 多模态数据处理</a></li>
<li><a class="reference internal" href="#id8">3. 奖励计算引擎</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9">架构组件</a><ul>
<li><a class="reference internal" href="#id10">类层次结构</a></li>
<li><a class="reference internal" href="#id11">关键类</a><ul>
<li><a class="reference internal" href="#id12">1. FastExperienceMaker</a></li>
<li><a class="reference internal" href="#multimodaldataprocessor">2. MultimodalDataProcessor</a></li>
<li><a class="reference internal" href="#rewardcomputationengine">3. RewardComputationEngine</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id13">使用指南</a><ul>
<li><a class="reference internal" href="#id14">基础用法</a><ul>
<li><a class="reference internal" href="#id15">纯文本生成</a></li>
<li><a class="reference internal" href="#id16">视觉-语言生成</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">高级用法</a><ul>
<li><a class="reference internal" href="#id18">多奖励模型与自定义聚合</a></li>
<li><a class="reference internal" href="#id19">样本打包以提高效率</a></li>
<li><a class="reference internal" href="#id20">远程奖励模型</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id21">配置参数</a><ul>
<li><a class="reference internal" href="#id22">生成参数</a></li>
<li><a class="reference internal" href="#id23">奖励处理参数</a></li>
<li><a class="reference internal" href="#id24">优势估计参数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id25">优势估计方法</a><ul>
<li><a class="reference internal" href="#gae">1. GAE（广义优势估计）</a></li>
<li><a class="reference internal" href="#rloo-reinforce-leave-one-out">2. RLOO（REINFORCE Leave-One-Out）</a></li>
<li><a class="reference internal" href="#reinforce-with-baseline">3. REINFORCE with Baseline</a></li>
<li><a class="reference internal" href="#group-normalization-grpo">4. Group Normalization (GRPO)</a></li>
<li><a class="reference internal" href="#id26">方法对比表</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id27">最佳实践</a><ul>
<li><a class="reference internal" href="#id28">1. 选择推理后端</a></li>
<li><a class="reference internal" href="#id29">2. 多模态数据处理</a></li>
<li><a class="reference internal" href="#id30">3. 奖励模型配置</a></li>
<li><a class="reference internal" href="#id31">4. 内存优化</a></li>
<li><a class="reference internal" href="#id32">5. 奖励归一化策略</a></li>
<li><a class="reference internal" href="#id33">6. 处理过长序列</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id34">常见问题与解决方案</a><ul>
<li><a class="reference internal" href="#oom">问题 1：内存不足（OOM）</a></li>
<li><a class="reference internal" href="#id35">问题 2：生成速度慢</a></li>
<li><a class="reference internal" href="#id36">问题 3：训练不稳定</a></li>
<li><a class="reference internal" href="#vlm">问题 4：图像令牌不匹配（VLM）</a></li>
<li><a class="reference internal" href="#rloo">问题 5：RLOO 需要多个样本</a></li>
<li><a class="reference internal" href="#id37">问题 6：远程奖励模型超时</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id38">性能调优</a><ul>
<li><a class="reference internal" href="#id39">吞吐量优化</a></li>
<li><a class="reference internal" href="#id40">内存效率</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id41">参考资料</a><ul>
<li><a class="reference internal" href="#id42">相关文档</a></li>
<li><a class="reference internal" href="#id43">代码引用</a></li>
<li><a class="reference internal" href="#id44">研究论文</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
    </section>
  </div>

  


  

  
  <script type="text/javascript" id="documentation_options" data-url_root="../"
    src="../_static/documentation_options.js"></script>
  <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
  <script src="../_static/doctools.js"></script>
  <script src="../_static/sphinx_highlight.js"></script>
  <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
  

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
    jQuery(function () {
      SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
  </div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://di-engine-docs.readthedocs.io/en/latest/" aria-label="OpenMMLab"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://github.com/opendilab/LightRFT" target="_blank">GitHub</a>
          </li>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function () {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function (e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>

</html>