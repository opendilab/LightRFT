v0.1.1 (2026.01.23)
- Add video support for reinforcement finetuning (#4)
- Add aime24/25 and gpqa_diamond evaluation support (#27)
- Implement and optimize evaluation for SRM and GRM trainers (#12)
- Add high entropy token selection mechanism (#6)
- Add analysis metrics to saved trajectories (#5)
- Adapt to latest versions of sglang, vllm, and deepspeed (#24)
- Resolve video metadata compatibility bugs (#25)
- Rename dtype to torch_dtype for better transformers compatibility (#7)
- Fix bug in GRM dataset message formatting and evaluation logic (#8)
- Remove redundant tuple nesting in prepare_reward_model return when using FSDP (#15)
- Fix make fcheck in lightrft/datasets for linting errors (#10)
- Add issue and PR templates (#20)
- Setup documentation deploy actions (#18)
- Update Python typing lint (#26)
- Polish API comment doc details (#21)
- Update GRM on T2I benchmark results and analysis in best practices (#9)
- Update general documentation and README for v0.1.1 (#2)

v0.1.0 (2025.12.24)
- initial public commit
